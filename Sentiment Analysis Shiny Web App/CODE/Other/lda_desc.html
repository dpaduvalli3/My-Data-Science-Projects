
<html>
<head>
<style>
h3, h3, h3 {
    text-align: left;
    font-weight:bold;}
    

</style>
</head>
<body>

<h3>About Latent Dirichlet allocation (LDA)</h3>
<p>Latent Dirichlet allocation is one of the most common algorithms for topic modeling. It is an unsupervised learning method similar to cluster analysis (where we discover latent groups or clusters).</p>

<h3>LDA is based on two guiding principles: </h3>
<p>
1.Every document is a mixture of topics: A document may contain words from several topics in particular proportions.For example, in a two-topic model we could say “Document 1 is 90% topic A and 10% topic B, while Document 2 is 30% topic A and 70% topic B.”<br/><br/>

2.Every topic is a mixture of words: We could imagine a two-topic model of American news, with one topic for “politics” and one for “entertainment.” The most common words in the politics topic might be “President”, “Congress”, and “government”, while the entertainment topic may be made up of words such as “movies”, “television”, and “actor”. Importantly, words can be shared between topics; a word like “budget” might appear in both equally. <br/><br/>

LDA is a mathematical method for estimating both of these at the same time: finding the mixture of words that is associated with each topic, while also determining the mixture of topics that describes each document.<br/><br/>

(Note: When viewing our visualizations please allow a few seconds for it to render)<br/><br/>

(Source: Silge, J., & Robinson, D. (2018, April 02). Text Mining with R. Retrieved May 4, 2018, from https://www.tidytextmining.com/tidytext.html)

</p>



</body>
</html>